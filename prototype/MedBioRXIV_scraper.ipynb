{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from p_tqdm import p_map\n",
    "import numpy as np\n",
    "import string\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important infos**\n",
    "\n",
    "- [x] Authors\n",
    "- [x] Publication Date\n",
    "- [x] Abstract\n",
    "- [ ] Full text link -> full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%file medbiorxivScraper.py\n",
    "class MedBioRxivScraper(object):\n",
    "    \n",
    "    base_url =  \"https://www.medrxiv.org/search/\"\\\n",
    "                \"%s%%20jcode%%3Amedrxiv%%7C%%7Cbiorxiv%%20\"\\\n",
    "                \"numresults%%3A%s%%20sort%%3A%s%%20format_result%%3Astandard\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    sort = {\n",
    "        'best':'relevance-rank',\n",
    "        'new':'publication-date%20direction%3Adescending',\n",
    "        'old':'publication-date%20direction%3Aascending'\n",
    "    }\n",
    "    \n",
    "    doi_re = re.compile('https://doi\\.org.*\\d')\n",
    "    remove_html_re = re.compile('<[^<]+?>')\n",
    "    \n",
    "    def search(self, \n",
    "                 search_term,\n",
    "                 no_results=1000,\n",
    "                 sort='best'\n",
    "                 ):\n",
    "        \n",
    "        self.search_term = search_term\n",
    "        self.url = self.base_url % (search_term.replace(' ', '%20'), no_results, self.sort[sort])\n",
    "        print('Waiting for server response ...', end=' ')\n",
    "        html = get(self.url).content\n",
    "        print('Done.')\n",
    "        print('Parsing HTML ...', end=' ')\n",
    "        self.soup = BeautifulSoup(html, features='lxml')\n",
    "        print('Done.')\n",
    "        print('Extracting DOIs...')\n",
    "        self.get_dois()\n",
    "        if len(self.DOIs)==no_results:\n",
    "            print('###! There might be more results available !###')\n",
    "        \n",
    "    def get_dois(self, regex = 'https://doi\\.org.*\\d'):\n",
    "        \n",
    "        self.DOIs = re.compile(regex).findall(self.soup.text)\n",
    "        print('No. of results: ', len(self.DOIs))\n",
    "        \n",
    "        \n",
    "    def parse(self, n_jobs=12):\n",
    "        \n",
    "        if n_jobs > len(self.DOIs):\n",
    "            n_jobs = len(self.DOIs)\n",
    "            \n",
    "            \n",
    "        with Pool(processes=n_jobs) as pool:\n",
    "            data = tqdm(\n",
    "                pool.map(\n",
    "                    MedBioRxivScraper.parse_article,\n",
    "                    self.DOIs,\n",
    "                ),\n",
    "                total = len(self.DOIs)\n",
    "            )\n",
    "#         data = p_map(MedBioRxivScraper.parse_article, self.DOIs, num_cpus=n_jobs)\n",
    "        self.data = pd.DataFrame(data, columns = ['authors', 'affiliations',\n",
    "                                                  'title', 'pub_date', 'abstract', 'doi'])\n",
    "        \n",
    "        self.data.pub_date = pd.to_datetime(self.data.pub_date)\n",
    "        \n",
    "    \n",
    "    @classmethod\n",
    "    def parse_article(cls, doi):\n",
    "        \n",
    "        \n",
    "        res = get(doi)\n",
    "        temp_soup = BeautifulSoup(res.content, features='lxml')\n",
    "        \n",
    "        authors = temp_soup.find_all('meta', {'name':['citation_author']})\n",
    "        authors = ';'.join([author['content'].strip(string.punctuation) for author in authors])\n",
    "        \n",
    "        affil = temp_soup.find_all('meta', {'name':['citation_author_institution']})\n",
    "        affil = ';'.join(np.unique([aff['content'].strip(string.punctuation) for aff in affil]))\n",
    "\n",
    "#         authors_string = ';'.join(['%s (%s)' % (x[0], x[1]) for x in zip(authors, affil)])\n",
    "        \n",
    "        try:\n",
    "            pub_date = temp_soup.find_all('meta', {'name':'article:published_time'})[0]['content']\n",
    "        except:\n",
    "            pub_date = '1900-01-01'\n",
    "        try:\n",
    "            title = temp_soup.find_all('meta', {'name':'citation_title'})[0]['content']\n",
    "        except:\n",
    "            title = 'none'\n",
    "        try:\n",
    "            abstract = temp_soup.find_all('meta', {'name':'citation_abstract'})[0]['content']\n",
    "            abstract = cls.remove_html_re.sub('', abstract)\n",
    "        except:\n",
    "            abstract = 'none'\n",
    "        \n",
    "        return authors, affil, title, pub_date, abstract, doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###!  No query specified - Aborting !###\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    def main():\n",
    "    \n",
    "        parser = argparse.ArgumentParser()\n",
    "\n",
    "        parser.add_argument('-q', '--query', type=str, help='phrase for search query')\n",
    "        parser.add_argument('-nr', '--no_results', type=int, help='max. number of results to retrieve')\n",
    "        parser.add_argument('-p', '--processes',type=int, help='number of processes to use for parsing')\n",
    "        parser.add_argument('-f', '--file', type=str, help='output file')\n",
    "        parser.add_argument('-s', '--sort', type=str, help='Sort by best, old, new')\n",
    "\n",
    "        args = parser.parse_args()\n",
    "\n",
    "        if not args.query:\n",
    "            print('###!  No query specified - Aborting !###')\n",
    "            return\n",
    "\n",
    "        s = MedBioRxivScraper()\n",
    "        \n",
    "        search_args = {\n",
    "            'no_results':1000,\n",
    "            'sort':'best'\n",
    "        }\n",
    "        \n",
    "        for arg in ('no_results', 'sort'):\n",
    "            if not getattr(args, arg) == None:\n",
    "                search_args[arg] = getattr(args, arg)\n",
    "\n",
    "        s.search(args.query, **search_args)\n",
    "\n",
    "        if args.processes:\n",
    "            s.parse(n_jobs=args.processes)\n",
    "        else:\n",
    "            s.parse()\n",
    "\n",
    "        filename = 'output.csv'\n",
    "        if args.file:\n",
    "            filename = args.file\n",
    "\n",
    "        s.data.to_csv(filename, index=False)\n",
    "\n",
    "        print('### Finished')\n",
    "        \n",
    "        return\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
